{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "from langchain.schema import HumanMessage, AIMessage, BaseChatMessageHistory\n",
    "from langchain_community.chat_message_histories.in_memory import ChatMessageHistory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See you next time!\n"
     ]
    }
   ],
   "source": [
    "def run_chatbot():\n",
    "    key = os.getenv(\"GROQ_API_KEY\")\n",
    "    llm = ChatGroq(\n",
    "        groq_api_key = key,\n",
    "        model=\"mixtral-8x7b-32768\",\n",
    "        temperature=0,\n",
    "        max_tokens=None,\n",
    "        timeout=None,\n",
    "        max_retries=2,\n",
    "    )\n",
    "    system_message = SystemMessagePromptTemplate.from_template(\n",
    "        \"You are a helpful assistant. Analyze the following query and provide a response with context. Do not be too verbose. Give a short and brief answer. Store memory of previous question.\"\n",
    "    )\n",
    "    human_message = HumanMessagePromptTemplate.from_template(\"{query}\")\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([system_message, human_message])\n",
    "    llm_chain = LLMChain(\n",
    "        prompt=chat_prompt,\n",
    "        llm=llm,\n",
    "    )\n",
    "    def chatbot(query):\n",
    "        response = llm_chain.run({\"query\": query})\n",
    "        return response\n",
    "    store = {}\n",
    "    def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "        if session_id not in store:\n",
    "            store[session_id] = ChatMessageHistory()\n",
    "        return store[session_id]\n",
    "    chain = llm_chain  \n",
    "    \n",
    "    with_message_history = RunnableWithMessageHistory(\n",
    "        chain,\n",
    "        get_session_history,\n",
    "        input_messages_key=\"query\",\n",
    "    )\n",
    "    session_id = \"abc20\" \n",
    "    config = {\"configurable\": {\"session_id\": session_id}}\n",
    "    while True:\n",
    "        user_query = input(\"You: \")\n",
    "        if user_query.strip().lower() == \"bye\":\n",
    "            print(\"See you next time!\")\n",
    "            break\n",
    "        history = get_session_history(session_id)\n",
    "        response = with_message_history.invoke(\n",
    "            {\n",
    "                \"query\": [HumanMessage(content=user_query)],\n",
    "            },\n",
    "            config=config,\n",
    "        )\n",
    "        if 'text' in response:\n",
    "            history.add_message(AIMessage(content=response['text']))\n",
    "            print(\"Assistant:\", response['text'])\n",
    "        else:\n",
    "            print(\"Assistant: Sorry, I don't understand.\")\n",
    "        \n",
    "       \n",
    "if __name__ == \"__main__\":\n",
    "    run_chatbot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
